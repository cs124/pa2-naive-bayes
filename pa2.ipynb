{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upy_H2JTdwyf"
   },
   "source": [
    "# CS 124 Programming Assignment 2: Naive Bayes (`Winter 2023`)\n",
    "\n",
    "Now that you are familiar with `Jupyter Notebooks` and have had some experience working with text data, it's time to begin investigating some real `Natural Language Processing` (`NLP`) tasks, specifically those requiring text classification.\n",
    "Text classification tasks come up in a huge range of contexts; quite often we may be provided with text data in some form and be interested in labeling or categorizing it in some way.\n",
    "In this assignment, you will be working on classifying messages for disaster aid using a `Naive Bayes` (`NB`) classifier, which we have covered in the course videos this week.\n",
    "In the next assignment, you will be asked to perform the same task using a `Logistic Regression` (`LR`) classifier.\n",
    "Victims of natural disasters have urgent needs for food, water, shelter, medicine, and other forms of aid.\n",
    "These needs are often communicated through text messages, social media posts, and local newspapers. Because of their\n",
    "ability to automatically process large amounts of text, `NLP` techniques can play an important role in ensuring that people receive potentially life-saving aid.\n",
    "Our goal will be to perform text classification on messages sent in the aftermath of natural disasters.\n",
    "After you are done testing your `NB` classifier on the disaster aid classification task, you will adapt it to another classification task: labelling the sentiment of messages related to `COVID` as positive or negative.\n",
    "\n",
    "We will be utilizing a `Python` module called `NumPy` in this and the next assignment.\n",
    "We are providing you with a `NumPy` tutorial (`numpy_tutorial.ipynb`) along with this assignment, which you can find in the same repository as this notebook. \n",
    "You can use the provided tutorial to learn `NumPy` for the first time or just refresh your knowledge.\n",
    "\n",
    "**You are encouraged to work with a partner!** We want the assignments in `CS 124` to bring you joy.\n",
    "One way to ensure this is to work with a partner!\n",
    "You are free to work with one other partner in our assignments.\n",
    "If you choose to work with a partner, we ask that each partner work on each part of the assignment in jointly instead of splitting parts.\n",
    "The partnership decision is independent for each assignment, so you can choose to work alone, work with the same partner or work with a different partner in the future assignments, which is a good way to meet your fellow classmates!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0T5kYg_3Y7S"
   },
   "source": [
    "<a id=\"contents\"></a>\n",
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DXv8PA2-hU5"
   },
   "source": [
    "Listed below are the contents of the assignment. In the `Data Exploration` \n",
    "section, you will look into the Disaster Aid Classification (`Triage`) dataset \n",
    "we will use in the assignment.\n",
    "In the `Naive Bayes` section, you will implement a `Naive Bayes` classifier to \n",
    "determine whether a message sent in the aftermath of a natural disaster is \n",
    "about aid.\n",
    "In the `Tips` section, we share some useful tips for your implementation.\n",
    "In the `Evaluation on the Triage Dataset` section, you will evaluate your \n",
    "`Naive Bayes` classifier on the `Triage` dataset.\n",
    "We will do the same on the `COVID` dataset in the \n",
    "`Evaluation on the COVID Dataset` section. \n",
    "Please read through all of this notebook before you start working through the assignment.\n",
    "**Note:** The links may not work on `Google Colab`.\n",
    "\n",
    "* [`Part 1. Data Exploration`](#data_exploration)\n",
    "* [`Part 2. Naive Bayes`](#naive_bayes)\n",
    "* [`Part 3. Tips`](#tips)\n",
    "* [`Part 4. Evaluation on the Triage Dataset`](#evaluation_triage)\n",
    "* [`Part 5. Evaluation on the COVID Dataset`](#evaluation_covid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pBXnsqTqHu6"
   },
   "source": [
    "<a id=\"roadmap\"></a>\n",
    "## Roadmap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwzw4SXt-tJy"
   },
   "source": [
    "As an overview, there are `4` methods you need to implement in this assignment:\n",
    "* In `Part 2. Naive Bayes`: **`__init__()`**, **`train()`**, **`classify()`**, and **`get_vocab_probabilities()`** of the **[`NaiveBayesClassifier(Classifier)`](#naive_bayes)** class\n",
    "\n",
    "Here is how your implementation will be evaluated:\n",
    "* In `Part 4. Evaluation on the Triage Dataset`, your \n",
    "  implementation will be evaluated with respect to the `triage` dataset.\n",
    "  * In `Part 4.1 Accuracy`, you will check the accuracy of your `Naive Bayes` \n",
    "    model both on the `train` and `dev` sets, each with and without stop words.\n",
    "    Hence, there will be a total of `4` accuracy tests in this section.\n",
    "    We recommend going back to your implementation if the accuracies you get in \n",
    "    this section are far from what we have provided.\n",
    "    In addition to what you will see in this section, our autograder will run  \n",
    "    `2` more tests on the same dataset, this time using a `test` set, with and\n",
    "    without stop words.\n",
    "    To see the autograder tests, you can submit your notebook on `Gradescope`.\n",
    "  * In `Part 4.2 Sanity Check`, we will run a few sanity checks on your \n",
    "    implementation.\n",
    "    We will only use the `sanity_check_vocab_probabilities()` method defined in \n",
    "    this section to evaluate your implementation in this section. \n",
    "* In `Part 5. Evaluation on the COVID Dataset`, we will run the same evaluations\n",
    "  as in `Part 4`, with the only difference being the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4gZzwRnPpWw"
   },
   "source": [
    "<a id=\"submitting\"></a>\n",
    "## Submitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuKzBe8IYqBT"
   },
   "source": [
    "**Submit your empty assignment to Gradescope now to see the autograder output!**\n",
    "You will submit your assignment via [`Gradescope`](www.gradescope.com), where we have an autograder set up.\n",
    "You can name your leaderboard submission whatever you would like!\n",
    "You can submit your assignment any number of times before the deadline.\n",
    "As a general rule of thumb, we recommend submitting early and often in any `Computer Science` class if you have the option, to prevent any last minute errors with autograders.\n",
    "Submitting early also helps gauge how you are doing on the visible test cases of the autograder and gives you a chance to fix your submission accordingly.\n",
    "In fact, start with submitting your assignment now (even if you haven't coded anything), so that you are familiar with the submission process and know what kind of autograder feedback is available to you.\n",
    "You can re-submit as you make progress.\n",
    "Don't forget to update your submission with your final version once you are done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y15qB5bxwR3N"
   },
   "source": [
    "**Partners.**\n",
    "You are welcome (and encouraged) to work with one partner.\n",
    "If you do work with a partner, only one of you needs to submit the assignment on `Gradescope` and tag the other as a group member."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCMk0AEJYupX"
   },
   "source": [
    "**Environment.**\n",
    "Before you submit, make sure your code works in the environment described in the [`Environment Check`](#environment_check) section, as this is the environment our autograder will be run on.\n",
    "If you have completed the setup steps in `PA0` and run this notebook in the `cs124` environment you created according to the instructions, you are good!\n",
    "Note that you must not use any other dependencies (such as other `Python` modules), as doing so may cause the autograder to fail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DjXAbXQYxcD"
   },
   "source": [
    "**Saving Your Notebook**.\n",
    "Make sure to save the recent changes in your notebook before you submit.\n",
    "This is especially important if you are running your notebook on `Google Colab` as connection quality sometimes cause your notebook to be in an unsaved state.\n",
    "The following error is also common on `Google Colab`, if the file you are working on is open in more than one tabs, so we are recommending keeping copies of your work if you are collaborating with your partner on `Colab`.\n",
    "```\n",
    "This file was updated remotely or in another tab. To force a save, overwriting the last update, select Save from the File menu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMSPTRReQFXo"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "**Files.**\n",
    "Once you are done, you only need to submit the file listed below.\n",
    "**DO NOT** alter the file name.\n",
    "```\n",
    "pa2.ipynb\n",
    "```\n",
    "\n",
    "**Custom Dependencies.**\n",
    "Sometimes you may want to put parts of your code into `.py` files and call them from your notebook instead of having all your functions in the notebook, or utilize extra datasets.\n",
    "If this is the case, please put your extra files in a folder\n",
    "named `deps/` (this folder should be on the same level as `pa2.ipynb`)\n",
    "and upload a `zip` file (any name is fine) containing this folder and\n",
    "`pa2.ipynb` to submit on `Gradescope`.\n",
    "Note that these should be at the top directory of the `.zip` file (e.g. they should not be in a directory in the `.zip` file, as this will lead our autograder to fail at finding them).\n",
    "To prevent this, ensure that you are only zipping the items mentioned, and not the folder containing them.\n",
    "`Gradescope` will then automatically `unzip` the folder so that your\n",
    "submission contains the following.\n",
    "```\n",
    "deps/\n",
    "pa2.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRZdYG4raTMj"
   },
   "source": [
    "**Submission Script.**\n",
    "For your convenience, we are providing the following submission script that lets you automatically create a `zip` file to submit.\n",
    "Simply run it and submit `submission.zip` to `Gradescope`.\n",
    "Note that the script assumes that you have the `zip` utility installed.\n",
    "You would need to install it if you don't already have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3UmC4xynbYbc",
    "outputId": "0d94f413-1c61-41ea-84b8-938461060afe",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./pa2.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. This probably means you're running on Google Colab. You'll need to go to File->Download .ipynb to download your notebok and other files, then zip them locally. See the README for more information.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip pa2.ipynb deps/\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC7Ji3nxbo2Z"
   },
   "source": [
    "If you are running your notebook on `Google Colab`, see the `README` for instructions on how to submit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mHHWQZvAZPZ4"
   },
   "source": [
    "**Autograder.**\n",
    "Once you submit, double check the autograder output to ensure that your submission didn't cause any error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnnkT9eMhBlJ"
   },
   "source": [
    "<a id=\"environment_check\"></a>\n",
    "## Environment Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cyv1CAE-o-m"
   },
   "source": [
    "This assignment assumes that you have correctly set up the `cs124` conda environment and installed the required `Python` modules.\n",
    "The cell below checks that you are running the correct version of `Python` and activated the `cs124` conda environment.\n",
    "If you are running the notebook on `Google Colab`, you need to download the `Python` extra modules we use in the assignment separately.\n",
    "If you get an error running this cell, it means that you are either using the wrong `Conda` environment\n",
    "or Python version!\n",
    "If the latter, please exit this notebook, kill the notebook server with `CTRL-C`, and\n",
    "try running:\n",
    "\n",
    "`$ conda activate cs124`\n",
    "\n",
    "Then restarting your notebook server with\n",
    "\n",
    "`$ jupyter notebook`\n",
    "\n",
    "If this doesn't work, you should go back and follow the installation instructions in `PA0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUVLxfnHiPbK",
    "tags": [
     "required"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs124\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdAYv1X28NIT"
   },
   "source": [
    "<a id=\"setup\"></a>\n",
    "## Part 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQ8pMJZJDIJu"
   },
   "source": [
    "**Getting the Necessary Files.** The cell below downloads the necessary files we will use in this assignment, if you don't already have them.\n",
    "This may be the case, for example, if you are running the assignment on `Google Colab`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LkwF0Sb28d7-",
    "outputId": "46d0e929-b287-405c-acbb-5ead463a9077",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -d \"./data\" ]]\n",
    "then\n",
    "    echo \"Missing extra files. Downloading...\"\n",
    "    git clone https://github.com/cs124/pa2-nb.git\n",
    "    cp -r ./pa2-naive-bayes/{data,deps,util.py} .\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dSKvfgi-Mna"
   },
   "source": [
    "**Importing Modules.** Run the next cell to import the necessary modules we will use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhuxbSxv-99D",
    "tags": [
     "required"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" Modules included in the Python Standard Library \"\"\"\n",
    "\n",
    "# collections module contain useful Python data structures, such as dictionaries\n",
    "# with special properties\n",
    "from collections import defaultdict\n",
    "\n",
    "# operator module allows us to use functions such as add() instead of operators\n",
    "# such as +\n",
    "import operator\n",
    "\n",
    "# random modules allows us to insert randomization to our code\n",
    "import random\n",
    "\n",
    "# typing module contains type objects. We will use these types to ensure that \n",
    "# the inputs and outputs passed to the functions you will be implementing are \n",
    "# of the correct type\n",
    "from typing import List, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXB-ukJxVXMj",
    "tags": [
     "required"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" Third party modules \"\"\"\n",
    "\n",
    "# numpy is a widely used scientific computing package, allowing us to do large\n",
    "# matrix operations efficiently\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib is a popular library used by researchers to plot graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn is a popular machine learning library, providing useful tools for \n",
    "# machine learning tasks\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5qPbMwQVC_a",
    "tags": [
     "required"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" Our custom functions and classes \"\"\"\n",
    "\n",
    "# Helper functions and classes we will use later\n",
    "from util import load_data, Classifier, Example, evaluate, remove_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ma5Fcxt9_X1d"
   },
   "source": [
    "**WARNING:** **DO NOT** import or use any other packages except the ones imported above and other packages in the Python standard library.\n",
    "This means you should not use `spaCy`, `NLTK`, `gensim`, or other functionality in `scikit-learn` besides `CountVectorizer`, even though those are provided in the `conda` environment we set up for you.\n",
    "If your solution uses any such extra dependencies it will fail the autograder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L98XWjQyjUw8"
   },
   "source": [
    "<a id=\"data_exploration\"></a>\n",
    "## Part 1. Data Exploration for the Triage Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8srZPWSq5IQ"
   },
   "source": [
    "As usual, the first thing to do is to understand and characterize the data!\n",
    "The data for this assignment contains about `26K` documents from several major natural disasters, as listed below.\n",
    "\n",
    "* [Earthquake in Haiti (2010)](https://en.wikipedia.org/wiki/2010_Haiti_earthquake)\n",
    "* [Floods in Pakistan (2010)](https://en.wikipedia.org/wiki/2010_Pakistan_floods)\n",
    "* [Earthquake in Chile (2010)](https://en.wikipedia.org/wiki/2010_Chile_earthquake)\n",
    "* [Hurricane Sandy in North America (2012)](https://en.wikipedia.org/wiki/Hurricane_Sandy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_u-uRvlRPhDP"
   },
   "source": [
    "**Dataset.** The documents in our dataset are either text messages, social media (`Twitter`) posts, or snippets from news articles.\n",
    "In addition to the specific events listed above the dataset contains a number of news articles spanning dozens of different disasters.\n",
    "All messages have been translated and annotated by humans on the crowdsourcing platform `CrowdFlower` (now branded under [`Appen`](https://appen.com/)).\n",
    "However, some of the translations are not perfect, and you may encounter some words in other\n",
    "languages.\n",
    "Unfortunately, `NLP` researchers often have to work with `messy` data.\n",
    "If you are curious about the crowdsourcing translation effort for messages\n",
    "from Haiti in particular, feel free to check out [this paper](https://nlp.stanford.edu/pubs/munro2010translation.pdf).\n",
    "\n",
    "Your task is to classify each document as being aid-related, class `aid`, or not aid-related, class `not`.\n",
    "Messages that are aid-related include individuals' requests for food, water, or shelter etc.\n",
    "The `aid` class also includes news reports about dire situations and disaster relief efforts.\n",
    "Below are several examples of aid-related documents, belonging to class `aid`.\n",
    "```\n",
    "Hello Good Morning We live on 31 Delmas we are without water without food and what we had have finished Please do something for us!\n",
    "```\n",
    "```\n",
    "I am sending this SMS from Layah district for my sister whose house has got destroyed in a flood\n",
    "So, the problem she faces now is that she hasn't got any 'Watan Card'or any financial aid from the government.\n",
    "She has 5 children too.\n",
    "```\n",
    "```\n",
    "Redcross came to my house and gave my family food ...\n",
    "Guess were not getting power anytime soon . #sandy #RedCross\n",
    "```\n",
    "```\n",
    "Relief officials have stressed the vital importance of bringing in clean drinking water and sanitation equipment to avoid deadly epidemics that in a\n",
    "worst case scenario could claim as many or more lives than the tsunami itself.\n",
    "```\n",
    "Below are several examples of non-aid-related documents, belonging to class `not`:\n",
    "```\n",
    "A cold front is found over Cuba this morning.\n",
    "It could cross Haiti tomorrow.\n",
    "Isolated rain showers are expected over our region tonight.\n",
    "```\n",
    "```\n",
    "Hurricane : A storm which New Yorkers use as an excuse to drink and eat junk\n",
    "food in their pajamas for 48 hours . #sandy\n",
    "```\n",
    "```\n",
    "By secret ballot, the Council elected Pakistan, Bahrain and the Republic of\n",
    "Korea from the Asian States, while Iran and Saudi Arabia did not receive enough\n",
    "votes to qualify.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCm95WnCcw-p"
   },
   "source": [
    "**Training, Validation, and Test Sets.**\n",
    "The data is divided into a `training` set, `development` (`validation`) set, and `test` set.\n",
    "Recall that the `training` set is used to learn, compute the statistics\n",
    "for, your model.\n",
    "These statistics are then used to classify the documents in the\n",
    "`development` and `test` sets.\n",
    "For this assignment, you have access to the\n",
    "`training` set and the `dev` set.\n",
    "The test `set` is hidden, but your submission will be evaluated on it as well.\n",
    "Although we do not share the specific test examples we use, the autograder we provide will output target accuracies your classifier should achieve for each of these sets for full credit. \n",
    "All you need to do to see these is to submit your assignment.\n",
    "Remember that you can always re-submit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2YYfAJYb5Bm"
   },
   "source": [
    "**Exploration.**\n",
    "Let's take a look at some of the data.\n",
    "We have defined a `Dataset` class for you to store the loaded data in a way we can easily access later, and a function `load_data()` to load it in the format we want.\n",
    "You do not need to check the specifics of our `Dataset` class, we will explain exactly how you will use it in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z4SSGY25eIoS",
    "outputId": "7019827b-9a67-4c91-938b-e0b7afb5030b",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# Load our dataset\n",
    "dataset = load_data(\"./data/triage\")\n",
    "\n",
    "# Check that the type of our dataset is the Dataset class we defined for you\n",
    "# in util.py.\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJDNkOtsgV66"
   },
   "source": [
    "We are interested in the following two fields of the `Dataset` class: `train` and `dev`.\n",
    "Given that `dataset` is an instance of the `Dataset` class, we can access these fields with `dataset.train` and `dataset.dev`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUgCeJsUgkRK",
    "outputId": "cb926386-3470-4e3d-da9a-67ca39794552",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "print(f\"dataset.train contains {len(dataset.train)} examples\")\n",
    "print(type(dataset.train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ut61mksEmR1r"
   },
   "source": [
    "Each of `dataset.train` and `dataset.dev` is a list of `Example`'s.\n",
    "Similar to `Dataset`, `Example` is a class we have defined to represent each data point we have.\n",
    "The `Example` class has two fields we will be concerned with: `words` and `label`.\n",
    "`words` field corresponds to the list of words making up the example.\n",
    "`label` field corresponds to the label of the data point, which is an integer that can only take one of two values: `1` for `aid` and `0` for `not` aid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwRvqrY5h6lz",
    "outputId": "72c843a9-7e61-47ec-9266-6b41c42f0c8e",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"First training example:\")\n",
    "print(\"Words: {}\".format(dataset.train[0].words))\n",
    "print(\"Label: {}\".format(dataset.train[0].label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lRaJN9dhorh"
   },
   "source": [
    "In summary, we use the custom defined classes `Example` and `Dataset` to represent our dataset and data points in a nice format so that we can work with them easily.\n",
    "This is achieved by the `load_data()` function we called earlier.\n",
    "At a high level, when we pass it the path `./data/triage`, `load_data()` finds the `CSV` files located there. \n",
    "Within each of these `CSV` files, each line is a single example, consisting\n",
    "of a document (`string`) and a corresponding label.\n",
    "The function `load_data()` reads each line in the `CSV` files as a new `Example`.\n",
    "It tokenizes each document it reads and sets the `words` field of the `Example` class to be a list of words.\n",
    "You can check the `CSV` files located in `./data/triage` to see the original format of these files.\n",
    "\n",
    "Note that the data you are given is already preprocessed; all punctuation has been removed, except hashtags and apostrophes, and all text has been converted to lowercase.\n",
    "If you were working on a new task, you would likely need to complete the preprocessing step yourself.\n",
    "Depending on the specific `NLP` task, preprocessing can significantly improve performance.\n",
    "You do not need to do any additional preprocessing for our task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94Qmp95Mc2d9"
   },
   "source": [
    "<a id=\"naive_bayes\"></a>\n",
    "## Part 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nfmLc4hpvCs"
   },
   "source": [
    "Now that we have our data set up, we can get started on implementing our `Naive Bayes` classifier!\n",
    "To help you, are are sharing a skeleton to get you started.\n",
    "Your job is to finish implementing the `NaiveBayesClassifier` class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfWXy7PkqK0S",
    "tags": [
     "required"
    ]
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier(Classifier):\n",
    "    \"\"\"\n",
    "    TODO: Implement the Multinomial Naive Bayes classifier with add-1 smoothing\n",
    "    (Laplace smoothing)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 filter_stop_words=False):\n",
    "        super().__init__(filter_stop_words)\n",
    "\n",
    "        # TODO: add other data structures needed in classify() or train()\n",
    "        # CODE START\n",
    "        pass\n",
    "        # CODE END\n",
    "\n",
    "    def train(self, examples: List[Example]) -> None:\n",
    "        \"\"\"\n",
    "        TODO: Implement a function that takes in a list of labeled\n",
    "        Examples and trains the classifier.\n",
    "\n",
    "        You can call the remove_stop_words function we imported earlier to\n",
    "        remove the stop words. \n",
    "        * List of stop words can be accessed with self.stop_words. \n",
    "        * self.filter_stop_words is a flag that specifies whether the stop words\n",
    "          should be removed.\n",
    "        \"\"\"\n",
    "        # CODE START\n",
    "        pass\n",
    "        # CODE END\n",
    "\n",
    "    def classify(self, examples: List[Example],\n",
    "                 return_scores: bool = False) -> Union[List[int], List[float]]:\n",
    "        \"\"\"\n",
    "        TODO: Implement a function that takes a list of Examples and predicts\n",
    "        their labels using the learned classifier.\n",
    "\n",
    "        If return_scores = True, return the score prob(label = 1 | example)\n",
    "        for each example instead.\n",
    "        \"\"\"\n",
    "        # CODE START\n",
    "        pass\n",
    "        # CODE END\n",
    "\n",
    "    def get_vocab_probabilities(self, label: int) -> Dict:\n",
    "        \"\"\"\n",
    "        TODO: Implement a function to return a dictionary of unigram ->\n",
    "        p(label | unigram) for every unigram in the vocabulary.\n",
    "        \"\"\"\n",
    "        # CODE START\n",
    "        pass\n",
    "        # CODE END\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39_k546BxrWr"
   },
   "source": [
    "The interface is very simple:\n",
    "\n",
    "* **`train()`** takes in a list of training `Example`s and updates the classifier based on the data. Note that you will want to save some information into the classifier class.\n",
    "* **`classify()`** takes a list of `Example`s, which will have labels, but\n",
    "you should not use them, and return a corresponding list of predicted labels\n",
    "(1 or 0) in the same order.\n",
    "If `return_scores = True`, it will return the score\n",
    "`prob(label = 1 | example)` for each example instead.\n",
    "* **`get_vocab_probabilities(label)`** should return a dictionary of `unigram ->\n",
    "p(label | unigram)` for every `unigram` in the vocabulary.\n",
    "\n",
    "Beyond these, everything else is up to you!\n",
    "You are free to add other helper methods or any other data structures and instance variables that you need.\n",
    "\n",
    "__WARNING:__ **DO NOT** change the interface of the methods listed above, as these will be called directly when grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQW1SySizC5o"
   },
   "source": [
    "<a id=\"tips\"></a>\n",
    "## Part 3. Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5luwDIfPzGJp"
   },
   "source": [
    "We do have some hints and suggestions that might help along\n",
    "the way before we jump to the evaluation section.\n",
    "It is totally possible to get a working solution without following all of these suggestions, so feel free to use or ignore them as you would like:\n",
    "\n",
    "**Use log probabilities.** We strongly recommend computing the probabilities as log probabilities in your implementation.\n",
    "Recall that your Naive Bayes prediction will be the argmax of a product of many probabilities (the prior probability $P(c)$ and a bunch of conditional probabilities $P(x | c)$ ).\n",
    "Each of these can individually be small numbers, so if you multiply many of them together, they may rapidly approach zero and even get rounded to 0, which will make it difficult or impossible for you to compare the actual values accurately.\n",
    "Instead, you can take the log of the probability, which will transform the\n",
    "product into a sum of logs. These will avoid any such bad behavior.\n",
    "And because log is a monotonically increasing function, if $log(x) > log(y)$ then $x > y$.\n",
    "So when computing your argmax, you can just compare the log probabilities \n",
    "directly and never need to worry about the true probabilities!\n",
    "\n",
    "**Keep track of the vocabulary.** For the purposes of implementing Laplace Smoothing (+1 smoothing), it may be\n",
    "helpful to keep track of the vocabulary, the set of all words you've\n",
    "seen in the training data, or at least its' size.\n",
    "The Python `set()` structure may be useful here.\n",
    "\n",
    "**You can use raw counts or computed probabilities.** There are a few different ways you can go about storing the information from\n",
    "learning in the `NaiveBayesClassifier` object.\n",
    "Ultimately, when classifying you will need to compute probabilities from the counts, but it's up to you whether you would like to store the raw counts or the computed probabilities.\n",
    "\n",
    "**Defaultdict!** You may find Python's [defaultdict](https://docs.python.org/3/library/collections.html#collections.defaultdict) helpful in your implementation when counting.\n",
    "\n",
    "**Consider the `remove_stop_words` flag.** Don't forget to implement your classifiers so they behave differently depending on if stop word filtering is enabled.\n",
    "You can filter stop words using the `remove_stop_words` function in `util.py`.\n",
    "If `filter_stop_words` is `True`, the `Classifier` will have a list of stop words stored in `self.stop_words`.\n",
    "\n",
    "**In Python, assignment is by reference.** Remember that in `Python`, assignment is by reference, not by value, for\n",
    "non-primitive types.\n",
    "Or to put things more simply, when you're assigning an existing list or dict to a new variable, it does NOT make a copy.\n",
    "It just gives a reference to the existing list or dict.\n",
    "\n",
    "  ```\n",
    "  a = [1, 2, 3]\n",
    "\n",
    "  # This does NOT make a copy of a. b now points to the same list as a\n",
    "  b = a\n",
    "\n",
    "  b.append(4)\n",
    "\n",
    "  # Prints \"[1, 2, 3, 4]\"\n",
    "  print(a)\n",
    "\n",
    "  # If you would like to make a copy of a list, you should do it explicitly\n",
    "  b = a.copy()\n",
    "\n",
    "  b.append(5)\n",
    "\n",
    "  print(b) # Prints \"[1, 2, 3, 4, 5]\"\n",
    "  print(a) # Prints \"[1, 2, 3, 4]\"\n",
    "  ```\n",
    "\n",
    "**Unknown words in test time.** Unknown words in the dev/test set that are not seen in your training set should be ignored in your Naive Bayes computations\n",
    "\n",
    "**Code length.** Our reference implementation is just under 100 lines of code, including the\n",
    "skeleton code.\n",
    "It's quite possible that you can make a working implementation in fewer lines (or more lines, that's totally fine too).\n",
    "But if your implementation is way longer than this, that might be a sign that you are over-complicating things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-ZY04F8zuVb"
   },
   "source": [
    "<a id=\"evaluation_triage\"></a>\n",
    "## Part 4. Evaluation on the Triage Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uizs3Cy0AN0"
   },
   "source": [
    "### Part 4.1. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYgmZfrMzXzS"
   },
   "source": [
    "Once your implementation is ready, you can try evaluating it on the disaster aid classification dataset as shown below.\n",
    "Our implementation achieves the following statistics, so if you are getting similar results that probably means that your implementation is working well!\n",
    "```\n",
    "Performance on Unigrams, no stopword removal:\n",
    "Accuracy (train): 0.82946878266654\n",
    "Accuracy (dev): 0.7329965021375826\n",
    "Performance on Unigrams with stopword removal:\n",
    "Accuracy (train): 0.8446735721752352\n",
    "Accuracy (dev): 0.7306645938593082\n",
    "```\n",
    "Our `autograder` will test the accuracy achieved by your implementation of the `NaiveBayesClassifier` on `train`, `dev`, and `test` datasets, both with and without stop words.\n",
    "If you aren't getting the performance you are expecting in the below cell, go back to your `train` and `classify` methods.\n",
    "Your implementation of `get_vocab_probabilities` doesn't affect the outputs of the two cells immediately below.\n",
    "If you are curious about the exact cutoffs we use to grade your work, you can submit your notebook to the autograder set up on `Gradescope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_yrkKIi2EKVy",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# Load our dataset\n",
    "dataset = load_data(\"./data/triage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4rzyTrkzVO9",
    "outputId": "c7dc7cdb-adb3-43d3-a570-e91da4f375f3",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Performance on unigrams, no stopword removal:\")\n",
    "nb_classifier = NaiveBayesClassifier(filter_stop_words=False)\n",
    "evaluate(nb_classifier, dataset)\n",
    "\n",
    "print(\"Performance on unigrams with stopword removal:\")\n",
    "nb_classifier_swr = NaiveBayesClassifier(filter_stop_words=True)\n",
    "evaluate(nb_classifier_swr, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNcYa1XGKIIG"
   },
   "source": [
    "### Part 4.2 Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cM2qYDkYKMtx"
   },
   "source": [
    "Once we've implemented and trained our model, it's often helpful to do some\n",
    "investigating to confirm that it's behaving the way we expect.\n",
    "For a Naive Bayes model, there are couple of different ways we can do this.\n",
    "\n",
    "**Proper probability distribution**. Given a word, we know that the vocabulary probabilities should add up to ~`1.0` when added across all the labels.\n",
    "Below two cells show this property for `k` words that are randomly chosen from our vocabulary.\n",
    "You may see that sometimes the sum of the probabilities isn't exactly `1.0`, such as `0.9999999999999999` or `1.0000000000000002`.\n",
    "This is fine, and even expected, as computers have a limited number of bits to hold numbers, which can lead to an overflow when we are dealing with floating point operations.\n",
    "Our autograder will check the probabilities computed by your `get_vocab_probabilities` implementation.\n",
    "Note that, every time you run the below cell, you will get a different set of `sample_words`.\n",
    "If you are getting unexpected values when you run the below two cells, go back to your `get_vocab_probabilities` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbG0DxmRKQc2",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "def sanity_check_vocab_probabilities(classifier, dataset):\n",
    "    # Preprocessing\n",
    "    labels = set(example.label for example in dataset.train)\n",
    "    vocab = set(word for example in dataset.train for word in example.words)\n",
    "    sample_words = random.sample(vocab, k=30)\n",
    "\n",
    "    # Check probabilities\n",
    "    for word in sample_words:\n",
    "        prob = 0\n",
    "        for label in labels:\n",
    "            prob += classifier.get_vocab_probabilities(label)[word]\n",
    "        if prob <= 0.999 or prob >= 1.001:\n",
    "            raise ValueError(f'The probabilities add up to {prob}')\n",
    "        print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mQVqBXWd4sm",
    "outputId": "28451d88-9048-472a-e35e-6beaf58aa232",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# Call sanity check\n",
    "sanity_check_vocab_probabilities(nb_classifier, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1PK0hTv0ceq"
   },
   "source": [
    "**Vocabulary Probabilities.**\n",
    "Another good sanity check is to examine the learned conditional probabilities for each of the words in the vocabulary.\n",
    "Specifically, we want to find the words for which the probability of a particular label is high.\n",
    "For example, to find the words that the model thinks best indicate an aid-related message, we would want to find words with a high value of $P(\\text{label} = 1 | \\text{word})$.\n",
    "We can do this easily using the `get_vocab_probabilities()` method.\n",
    "Let's print out the top `10` highest-probability words for each labe, `aid` and `not`.\n",
    "\n",
    "What did you think of the words you saw? \n",
    "Do they seem plausible to you? \n",
    "Note that we won't be testing the properties shared in the remainder of `Part 4` in our autograder, but we are sharing them here to help you understand how naıve bayes work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cU5nviYd5TGq",
    "outputId": "24ef3960-f95d-4b56-91d2-8d605de05e86",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# Words that best indicate `aid`\n",
    "vocab_probs_positive = nb_classifier.get_vocab_probabilities(1)\n",
    "top_10_positive = sorted(vocab_probs_positive.items(),\n",
    "                         key=operator.itemgetter(1), reverse=True)[:10]\n",
    "\n",
    "for word, prob in top_10_positive:\n",
    "    print(\"{}: prob = {}\".format(word, prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAC5uvZo5TGr",
    "outputId": "68a12e9b-73f7-40ea-ad7e-999eef8832e2",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# Words that best indicate not `aid`\n",
    "vocab_probs_negative = nb_classifier.get_vocab_probabilities(0)\n",
    "top_10_negative = sorted(vocab_probs_negative.items(),\n",
    "                         key=operator.itemgetter(1), reverse=True)[:10]\n",
    "\n",
    "for word, prob in top_10_negative:\n",
    "    print(\"{}: prob = {}\".format(word, prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve3QXPqc5TGr",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**False Positives and Negatives.** Another good thing to check is where your model made errors. In this case, our\n",
    "task was binary classification, so there are two possible types of errors\n",
    "that could have occurred:\n",
    "\n",
    "* `False Positives`: Our model predicted a high probability of label = 1 for a negative example.\n",
    "* `False Negatives`: Our model predicted a low probability of label = 1 for a positive example.\n",
    "\n",
    "We can look for exactly these two types of errors using the `return_scores`\n",
    "flag we asked you to implement for the `classify()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j95TGjLK5TGs",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "def get_false_negatives_and_false_positives(classifier, examples):\n",
    "    predicted_scores = classifier.classify(examples, return_scores=True)\n",
    "\n",
    "    false_negatives = []\n",
    "    false_positives = []\n",
    "\n",
    "    for pred_score, example in zip(\n",
    "            predicted_scores, examples):\n",
    "        if example.label == 1 and pred_score < 0.5:\n",
    "            false_negatives.append((example.words, pred_score))\n",
    "        elif example.label == 0 and pred_score >= 0.5:\n",
    "            false_positives.append((example.words, pred_score))\n",
    "\n",
    "    return false_negatives, false_positives\n",
    "\n",
    "fn, fp = get_false_negatives_and_false_positives(nb_classifier, dataset.dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HQabfgj1Iez"
   },
   "source": [
    "Now that we have the false negatives and false positives, we can find the\n",
    "\"worst\" ones and examine them to try to figure out where our model went wrong.\n",
    "\n",
    "The **worst** ones would be:\n",
    "* The `false negatives` with the lowest probabilities of label = 1\n",
    "* The `false positives` with the highest probabilities of label = 1\n",
    "\n",
    "Run the cells below, and think about the following questions:\n",
    "* Do the mistakes you see seem reasonable/sensible? Why do you think the classifier\n",
    "misclassified them?\n",
    "* Could some of these misclassifications be avoided if we had access to more\n",
    "training data? Or do some of them stem from the limitations of the Naive Bayes\n",
    "model itself (Hint: Think about what assumptions go into the Naive Bayes model)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogpOEF4c5TGs",
    "outputId": "c27a08fb-5eec-4165-9017-528e4da40800",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "top_10_fn = sorted(fn,\n",
    "                   key=operator.itemgetter(1))[:10]\n",
    "for words, prob in top_10_fn:\n",
    "    print(\"prob = {}: {}...\".format(prob, words[:min(len(words), 10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4weD966k5TGt",
    "outputId": "75725477-815e-486b-8a0f-5a28387c16b7",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "if len(fp) == 0:\n",
    "    print(\"No false positives found!\")\n",
    "\n",
    "top_10_fp = sorted(fp, key=operator.itemgetter(1), reverse=True)[:10]\n",
    "for words, prob in top_10_fp:\n",
    "    print(\"prob = {}: {}\".format(prob, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKRIDRHH5TGt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Hopefully these questions have gotten you thinking about what your model\n",
    "is doing and what its weaknesses and problems might be.\n",
    "If you have time, we encourage you to spend some more time playing around with your model in this section.\n",
    "Finally, when computing your model's performances with the different settings, no stop word removal vs. stop word removal,, you may have\n",
    "noticed something surprising.\n",
    "In general, we would expect models using stop word removal to outperform those that don't in many cases.\n",
    "However, you may have found that on this dataset, this does not necessarily\n",
    "occur!\n",
    "This does not mean that your implementation is broken or incorrect, although you should definitely double-check just to be sure.\n",
    "Why do you think this might be happening? What could be changed to get the\n",
    "expected behavior?\n",
    "\n",
    "**NOTE:** It may be helpful to consider the differences between the training\n",
    "set and dev accuracies. You can think back to our discussion of overfitting in the group work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujY6VITu1ugt"
   },
   "source": [
    "<a id=\"evaluation_covid\"></a>\n",
    "## Part 5. Evaluation on the COVID Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZ4MrUe810dR"
   },
   "source": [
    "Now that we have verified that our `Naive Bayes` model behave correctly on a given dataset, it is easy to apply it and evaluate on an alternative dataset and see if it perform differently.\n",
    "In `data/coronavirus`, we have provided a second dataset consisting of `reddit` comments on posts related to the `COVID-19` pandemic early last year.\n",
    "Each example is a single line in the `CSV` file, consisting of a comment and associated sentiment label: `0` for `negative`, and `1` for `positive`.\n",
    "This dataset consists of around `4` times as many examples as the `triage/disaster` dataset.\n",
    "The task on this dataset is to, given the text of a comment, predict its\n",
    "sentiment label.\n",
    "This is an example of sentiment analysis, specifically sentiment classification, which another type of `NLP` task.\n",
    "\n",
    "In this particular case, you could imagine using sentiment analysis tools\n",
    "to get an approximate idea of the mood social media, i.e. `Reddit`, users feel\n",
    "about the pandemic at a particular point in time.\n",
    "If we succeeded in training a good classifier to identify `positive` and `negative` sentiment comments/posts, we could use it to count the number of `positive` and `negative` posts each day, giving an approximate measure of social media sentiment.\n",
    "This information could be very useful for governments or NGOs trying to gauge\n",
    "the public response to `COVID-19` policies, or identify how the pandemic is\n",
    "affecting mental health.\n",
    "Although the task is different, we can straightforwardly load and examine the new data the same way we did with the previous dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Zrprs9l5TG3",
    "outputId": "21240497-e8dd-426a-ec85-3280eb95e9ab",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "# Load our dataset\n",
    "covid_dataset = load_data(\"./data/coronavirus\")\n",
    "print(type(covid_dataset))\n",
    "\n",
    "print(\"dataset.train contains {} examples\".format(len(covid_dataset.train)))\n",
    "\n",
    "print(\"First training example:\")\n",
    "print(\"Words: {}\".format(covid_dataset.train[0].words))\n",
    "print(\"Label: {}\".format(covid_dataset.train[0].label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7EZnj555TG3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can train and evaluate our models just like before as well. \n",
    "Our solution achieves the following statistics when we run the below cell, so if you are getting similar results that probably means that your implementation is working well!\n",
    "```\n",
    "Performance on unigrams, no stopword removal:\n",
    "Accuracy (train): 0.8691\n",
    "Accuracy (dev): 0.7853\n",
    "Performance on unigrams with stopword removal:\n",
    "Accuracy (train): 0.8632125\n",
    "Accuracy (dev): 0.7706\n",
    "```\n",
    "Our `autograder` will test the accuracy achieved by your implementation of the `NaiveBayesClassifier` on `train`, `dev`, and `test` subsets of the `COVID` dataset, both with and without stop words.\n",
    "\n",
    "**Food for thougth:** What do you notice about the results you got on this dataset compared to the previous one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHL-feO25TG3",
    "outputId": "64b74032-a086-4ad9-d4c6-e23b498f0c3b",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "print(\"Performance on unigrams, no stopword removal:\")\n",
    "nb_classifier = NaiveBayesClassifier(filter_stop_words=False)\n",
    "evaluate(nb_classifier, covid_dataset)\n",
    "\n",
    "print(\"Performance on unigrams with stopword removal:\")\n",
    "nb_classifier_swr = NaiveBayesClassifier(filter_stop_words=True)\n",
    "evaluate(nb_classifier_swr, covid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM8lqZQ75TG3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before we wrap up, let's run the quick probability sanity check our method `get_vocab_probabilities` using the `COVID` dataset.\n",
    "This will also be checked by our autograder! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNezpSQQgjMX",
    "outputId": "3556ff1f-51b6-4082-bc2d-2dbdd54f8d32",
    "tags": [
     "exploration"
    ]
   },
   "outputs": [],
   "source": [
    "sanity_check_vocab_probabilities(nb_classifier, covid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kK10Ni0iNdu"
   },
   "source": [
    "This is it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O2n-GyiIIVT"
   },
   "source": [
    "<a id=\"ending_remarks\"></a>\n",
    "## Ending Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pI3K3ewfb2g"
   },
   "source": [
    "Congratulations, you are done with the assignment!\n",
    "Refer to the [`Submitting`](#submitting) for submission instructions."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "pa2-solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
